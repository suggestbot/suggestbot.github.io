---
layout: default
title: {{ site.name }}
---

<h1>What is SuggestBot?</h1>
<p>SuggestBot is a software component that identifies the user's needs and suggests an action, which is the key aspect of "Smart Interaction". Two key elements of SuggestBot are "context awareness" and "real-time interaction".</p>

<h3>Support</h3>
<p style="font-size:0.9rem">This research was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT (2017M3C4A7065955)</p>
<p style="font-size:0.9rem">이 연구는 정부(과학기술정보통신부)의 재원으로 한국연구재단-차세대정보・컴퓨팅기술개발사업의 지원을 받아 수행된 연구임(2017M3C4A7065955).</p>
<p style="font-size:0.9rem">Period : 2017.09.01 - 2021.03.31</p>

<h2>Goals</h2>
<ol>
  <li>Development of a context inference engine to support SuggestBot's context awareness in various wearable computing scenarios</li>
  <li>Building labeled big data for researching context reasoning for context inference engine design</li>
  <li>Development of SuggestBot UI and service software subsystem to guarantee the real time requirement of SuggestBot</li>
</ol>

<h2>Contents</h2>
<table>
  <tbody>
    <!-- Data Set -->
    <tr>
      <td colspan=2 style="background-color:#e9ebec"><strong>DataSet</strong></td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="http://143.248.95.164:8080/"><strong>A dataset for the analysis of interest and affective state in exhibition visiting scenario</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">We present a multimodal dataset for the analysis of human interest and affective states. Physiological, motion, and video signals of 100 participants were recorded as each visited the exhibition room for at least 20 minutes.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://doi.org/10.5281/zenodo.3931963"><strong>K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        We present a multimodal dataset for the analysis of human emotion. EEG, physiological, motion, and video signals of 30 participants were recorded as each participated in the debate for at least 15 minutes.
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/kixlab/suggestbot_dataset"><strong>Online discussion and evidence dataset</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">From Internet Argument Corpus (IAC) 2.0 dataset, this dataset covers 10,000 posts containing claims that can be verified with facts. The dataset contains annotations on verifiable sentences for each post and factual information that supports or refutes each annotation.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://drive.google.com/drive/folders/1_u51hfJ_YQl-LCxQZ9bEKzfd0NDC1qcP?usp=sharing"><strong>K-EmoPhone, students' emotion and stress in daily life</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        We present a sensor dataset collected by wearable sensors and smartphones. This dataset contains physiological signals, motion/location-related records, behavior features with the corresponding emotion and stress labels.
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/kixlab/suggestbot-context-tag-dataset"><strong>Context-related Hashtag Classification Dataset</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This dataset contains the classification of hashtags from social media posts into eight context categories ("Emotion", "Mood", "Location", "Time", "Object", "Activity", "Event", "Other").
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/kixlab/suggestbot-psychological-safety-dataset"><strong>Psychological Safety Dataset</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This dataset contains annotations on the utterances from AMI Corpus that would affect the psychological safety of the group. Each annotation describe a) whether the utterance would positively or negatively affect the psychological safety of the group and b) how the annotator would explain their annotation to the meeting participants.
      </td>
    </tr>
    <!-- Data Collect Program -->
    <tr>
      <td colspan=2 style="background-color:#e9ebec"><strong>Data Collect Program</strong></td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/Kaist-ICLab"><strong>Sensor and interactive data logger for mobile and wearable devices</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">The application collects sensor and interactive data from mobile and wearable devices, store the data in the device, and sends to predefined servers.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/Kaist-ICLab"><strong>Heart rate information logger based on Ant+ protocol</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">The application detects and collects heart rate information via BLE (Bluetooth Low Energy) connection based on ANT+ protocol. ANT+ is a Wireless Personal Network protocol.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/Kaist-ICLab"><strong>Data logger library/application for Android devices</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">This is a simple, but powerful logger that allows Android apps to log both to the internal log, as well as an external file (i.e. in sqlite format). This can also be used as a stand-alone application.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/Kaist-ICLab/ABC-Logger"><strong>ABC Logger, for the collection of a variety of smartphone data</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This application collects various types of sensor data from an Android smartphone and the connected wearable sensors (e.g., Polar H10 chest-band), and the user's own status logs by using the experience sampling method (ESM).
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/Kaist-ICLab/SmartSpeakerESM"><strong>Smart Speaker ESM, for the collection of user contexts with using a voice-based experience sampling method</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This application collects the opportune moments in the domestic context for proactive conversational interactions with smart speakers.
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem">
        <a href="https://github.com/kixlab/suggestbot-instagram-context-annotator">
          <strong>
            Crowdsourcing Task for Collecting Hashtag-Context Mapping
          </strong>
        </a>
      </td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This crowdsourcing interface is designed to collect hashtags likely to be used in social media posts and classification of hashtags into context categories. The interface collects hashtags from worker's own social media posts with explicit agreement or hypothetical social media posts created by the worker with their own photos.
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem">
        <strong>
          Crowdsourcing Task for Collecting Annotations on Psychological Safety
        </strong>
        <a href="https://github.com/kixlab/suggestbot-ps-front">
          [Frontend]
        </a>
        <a href="https://github.com/kixlab/suggestbot-ps-backend">
          [API server]
        </a>
      </td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This crowdsourcing interface is designed to collect annotations on the utterances from meeting logs. The current version of the interface is targetted to collect utterances affecting the psychological safety of the group, and it could be applied to various other annotations.
      </td>
    </tr>
    <!-- Deep Learning Module -->
    <tr>
      <td colspan=2 style="background-color:#e9ebec"><strong>Deep Learning Module</strong></td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://gitlab.com/pjknkda/suggestbot-emotion-engine"><strong>Denoising Convolutional and Recurrent Neural Network for Multimodal Emotion Recognition</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This program incorporates multiple heterogeneous sensor data to recognize human emotion. The main machine learning model of the program pipelines convolutional neural networks and a recurrent neural network to effectively extract features from multimodal sensor data and recognize human emotion using the extracted features. To improve the model performance, various optimization techniques including the denoising gated recurrent unit (DGRUD) and the convolutional block attention module (CBAM) are applied. The repository also provides pre-trained models with a set of sensor configurations.
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/kyoungrok0517/suggestbot-demo"><strong>Search Engine for Claim-supporting Content with Deep Learning</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">We build a module that identifies the contents that can support(e.g. related, strengthen) a given claim. We use Wikipedia and news articles as main resources.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/jm-kang/suggestbot-tag-engine"><strong>Context-Aware Tag Prediction Model</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This tag engine is devised for recommending relevant tags. This exploits various context information such as image, location, and time to suggest a list of tags. The tags to be output are pre-defined based on how often they occur in the collected Instagram posts.
      </td>
    </tr>
    <!-- Platform System -->
    <tr>
      <td colspan=2 style="background-color:#e9ebec"><strong>Platform System</strong></td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://gitlab.com/spchoi/deepschedulerexperiment"><strong>TFLite based benchmark program for multiple DL model running situation on mobile</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">The benchmark tool measures DL model execution performance (e.g., latency) while multiple CNN models running simultaneousl. This program has been confirmed to work on a Pixel 2 device with Android 8.0 or higher. The benchmark currently offers two popular DL models, MobileNet_V1(quantized) and Inception_V3(quantized), which are officially supported by TensorFlow Lite.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://gitlab.com/taegyeong/privacy-preserving-dl"><strong>Privacy-preserving Deep Learning System</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        This repository contains an implementation of a privacy-preserving deep learning inference system for remote clouds or edge servers. The system prevents data disclosure and modification from malicious attackers by leveraging a trusted execution environment(TEE). The system is built upon Caffe, a popular deep learning framework, to port into Intel SGX enclaves. Since using shared libraries are not allowed in the enclaves, the repository includes all source files of Caffe and its dependencies.
      </td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/suggestbot-wearable-text-entry-system"><strong>Wearable Text entry system for smart glasses with a smartwatch as an input device</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">The wearable text entry system decomposed into 2 parts, Smart glasses as an output device (Display) and Smartwatch as an input device (Touch gestures). Both devices are connected with Bluetooth connection and both of the devices should be powered by Android OS (min SDK ver. is 19). There are embedded two different keyboards, one is SwipeBoard and the other is HoldBoard. In the case of HoldBoard, it is needed to use an edge-touch sensitive smartwatch, but it is more effective to enter a text.</td>
    </tr>
    <tr>
      <td style="font-size:1rem"><a href="https://github.com/suggestbot-wearable-text-entry-system"><strong>Gaze-Assisted Typing Interface for Smart Glasses</strong></a></td>
    </tr>
    <tr>
      <td style="font-size:0.9rem">
        We develop gaze-assisted typing (GAT) interface, which uses both a touchpad and eye tracking, for more efficient smart glass text entry.
      </td>
    </tr>
  </tbody>
</table>

<h3>Keywords</h3>
<blockquote>
  <p><em>Smart Interaction</em>, <em>SuggestBot</em>, <em>Context Big Data</em>, <em>Context Deep Learning</em>, <em>Wearable Computing Platform</em></p>
</blockquote>
